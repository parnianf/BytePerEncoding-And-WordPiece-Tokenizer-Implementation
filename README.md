# BytePerEncoding-And-WordPiece-Tokenizer-Implementation

Some of the popular subword-based tokenization algorithms are WordPiece, Byte-Pair Encoding (BPE), Unigram, and SentencePiece. The goal of this project is to get deeper into BPE and WordPiece.

### Datasets

http://www.gutenberg.org/cache/epub/16457/pg16457.txt

https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip

(Sample.txt)[https://github.com/parnianf/BytePerEncoding-And-WordPiece-Tokenizer-Implementation/blob/main/Sample.txt]

\* Make sure you have the Sample.txt file and the wikitext-103-raw folder to run the code.

## Step 1

* Explaning of BPE & WordPiece
* Comapring BPE & WordPiece
* Implementing BPE from scratch

## Step 2

* Implementing BPE & WordPiece by Hugging Face
* Applying these models on Gutenberg book & English Wikipedia & a sample text file given.
* Explaning & analyzing the differences

## Step 3

* Analyzing the results


## Report
Report is available [here](https://github.com/parnianf/BytePerEncoding-And-WordPiece-Tokenizer-Implementation/blob/main/NLP-CA1-Report-English.pdf).


